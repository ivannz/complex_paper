
@book{petersen_matrix_2012,
	title = {The {Matrix} {Cookbook}},
	abstract = {These pages are a collection of facts (identities, approximations, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference.},
	language = {en},
	publisher = {Technical University of Denmark},
	author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
	month = nov,
	year = {2012},
	keywords = {inverse, matrix derivative, Matrix identity, matrix relations},
	annote = {Version 20121115}
}

@inproceedings{trabelsi_deep_2018,
	title = {Deep {Complex} {Networks}},
	abstract = {At present, the vast majority of building blocks, techniques, and architectures for deep learning are based on real-valued operations and representations. However, recent work on recurrent neural networks and older fundamental theoretical analysis suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Despite their attractive properties and potential for opening up entirely new neural architectures, complex-valued deep neural networks have been marginalized due to the absence of the building blocks required to design such models. In this work, we provide the key atomic components for complex-valued deep neural networks and apply them to convolutional feed-forward networks and convolutional LSTMs. More precisely, we rely on complex convolutions and present algorithms for complex batch-normalization, complex weight initialization strategies for complex-valued neural nets and we use them in experiments with end-to-end training schemes. We demonstrate that such complex-valued models are competitive with their real-valued counterparts. We test deep complex models on several computer vision tasks, on music transcription using the MusicNet dataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve state-of-the-art performance on these audio-related tasks.},
	urldate = {2019-06-03},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, Jo√£o Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
	year = {2018},
	note = {arXiv: 1705.09792},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, printed}
}

@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2019-06-03},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Computer Science - Machine Learning, printed, Statistics - Machine Learning}
}

@article{lapidoth_capacity_2003,
	title = {Capacity bounds via duality with applications to multiple-antenna systems on flat-fading channels},
	volume = {49},
	doi = {10.1109/TIT.2003.817449},
	abstract = {A technique is proposed for the derivation of upper bounds on channel capacity. It is based on a dual expression for channel capacity where the maximization (of mutual information) over distributions on the channel input alphabet is replaced with a minimization (of average relative entropy) over distributions on the channel output alphabet. We also propose a technique for the analysis of the asymptotic capacity of cost-constrained channels. The technique is based on the observation that under fairly mild conditions capacity achieving input distributions "escape to infinity." The above techniques are applied to multiple-antenna flat-fading channels with memory where the realization of the fading process is unknown at the transmitter and unknown (or only partially known) at the receiver. It is demonstrated that, for high signal-to-noise ratio (SNR), the capacity of such channels typically grows only double-logarithmically in the SNR. To better understand this phenomenon and the rates at which it occurs, we introduce the fading number as the second-order term in the high-SNR asymptotic expansion of capacity, and derive estimates on its value for various systems. It is suggested that at rates that are significantly higher than the fading number, communication becomes extremely power inefficient, thus posing a practical limit on practically achievable rates. Upper and lower bounds on the fading number are also presented. For single-input-single-output (SISO) systems the bounds coincide, thus yielding a complete characterization of the fading number for general stationary and ergodic fading processes. We also demonstrate that for memoryless multiple-input single-output (MISO) channels, the fading number is achievable using beam-forming, and we derive an expression for the optimal beam direction. This direction depends on the fading law and is, in general, not the direction that maximizes the SNR on the induced SISO channel. Using a new closed-form expression for the expectation of the logarithm of a noncentral chi-square distributed random variable we provide some closed-form expressions for the fading number of some systems with Gaussian fading, including SISO systems with circularly symmetric stationary and ergodic Gaussian fading. The fading number of the latter is determined by the fading mean, fading variance, and the mean squared error in predicting the present fading from its past; it is not directly related to the Doppler spread. For the Rayleigh, Ricean, and multiple-antenna Rayleigh-fading channels we also present firm (nonasymptotic) upper and lower bounds on channel capacity. These bounds are asymptotically tight in the sense that their difference from capacity approaches zero at high SNR, and their ratio to capacity approaches one at low SNR.},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Lapidoth, Amos and Moser, Stefan M.},
	month = oct,
	year = {2003},
	keywords = {antenna arrays, antenna theory, asymptotic capacity, beam direction, beamforming, capacity bounds, channel capacity, Channel capacity, circularly symmetric stationary, circularly symmetric stationary fading, closed-form expression, Closed-form solution, cost-constrained channels, dual expression, duality, Entropy, ergodic fading processes, ergodic Gaussian fading, Fading, fading mean, fading variance, flat-fading channels, H infinity control, lower bounds, mean squared error, memoryless multiple-input single-output channels, MIMO systems, MISO channels, multiple-antenna systems, Mutual information, noncentral chi-square distributed random variable, Rayleigh channels, Ricean channels, Rician channels, second-order term, Signal to noise ratio, signal-to-noise ratio, single-input-single-output channel, SISO systems, stationary fading processes, Transmitters, Upper bound, upper bounds},
	pages = {2426--2467}
}

@incollection{novikov_tensorizing_2015,
	title = {Tensorizing {Neural} {Networks}},
	urldate = {2019-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {442--450}
}

@article{benvenuto_complex_1992,
	title = {On the complex backpropagation algorithm},
	volume = {40},
	issn = {1053-587X, 1941-0476},
	doi = {10.1109/78.127967},
	abstract = {A recursive algorithm for updating the coefficients of a neural network structure for complex signals is presented. Various complex activation functions are considered and a practical definition is proposed. The method, associated to a mean-square-error criterion, yields the complex form of the conventional backpropagation algorithm.{\textless}{\textgreater}},
	number = {4},
	journal = {IEEE Transactions on Signal Processing},
	author = {Benvenuto, N. and Piazza, F.},
	month = apr,
	year = {1992},
	keywords = {Backpropagation algorithms, coefficients updating, complex activation functions, complex backpropagation algorithm, complex signals, Equations, mean-square-error criterion, Multi-layer neural network, Multilayer perceptrons, neural nets, neural network, Neural networks, Neurons, Nonlinear filters, recursive algorithm, signal processing, Signal processing, Signal processing algorithms, Wiener filter},
	pages = {967--969}
}

@inproceedings{wan_regularization_2013,
	title = {Regularization of {Neural} {Networks} using {DropConnect}},
	abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations ar...},
	language = {en},
	urldate = {2019-11-05},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
	month = feb,
	year = {2013},
	pages = {1058--1066}
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	urldate = {2019-11-05},
	journal = {arXiv:1207.0580 [cs]},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	month = jul,
	year = {2012},
	note = {arXiv: 1207.0580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{wolter_complex_2018,
	address = {USA},
	series = {{NIPS}'18},
	title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
	abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
	urldate = {2019-11-22},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wolter, Moritz and Yao, Angela},
	year = {2018},
	note = {event-place: Montr√©al, Canada},
	pages = {10557--10567}
}

@inproceedings{hirose_complex-valued_2009,
	title = {Complex-valued neural networks: {The} merits and their origins},
	shorttitle = {Complex-valued neural networks},
	doi = {10.1109/IJCNN.2009.5178754},
	abstract = {This paper discusses what the merits of complex-valued neural networks (CVNNs) arise from. First we look back the mathematical history to elucidate the features of complex numbers, in particular to confirm the importance of the phase-and-amplitude viewpoint for designing and constructing CVNNs to enhance the features. The viewpoint is essential in general to deal with waves such as electromagnetic-wave and lightwave. Then we point out that, although we represent a complex number as an ordered pair of real numbers for example, we can reduce ineffective degree of freedom in learning or self-organization in CVNNs to achieve better generalization characteristics. This wave-oriented merit is useful widely for general signal processing with Fourier synthesis or in frequency-domain treatment through Fourier transform.},
	booktitle = {2009 {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Hirose, Akira},
	month = jun,
	year = {2009},
	note = {ISSN: 2161-4407},
	keywords = {complex numbers, complex-valued neural networks, electromagnetic-wave, Fourier synthesis, Fourier transform, Fourier transforms, frequency-domain analysis, frequency-domain treatment, general signal processing, neural nets, Neural networks, signal processing},
	pages = {1237--1244}
}

@inproceedings{arjovsky_unitary_2016,
	title = {Unitary {Evolution} {Recurrent} {Neural} {Networks}},
	abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to ...},
	language = {en},
	urldate = {2019-11-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
	month = jun,
	year = {2016},
	pages = {1120--1128}
}

@incollection{wisdom_full-capacity_2016,
	title = {Full-{Capacity} {Unitary} {Recurrent} {Neural} {Networks}},
	urldate = {2019-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Wisdom, Scott and Powers, Thomas and Hershey, John and Le Roux, Jonathan and Atlas, Les},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {4880--4888}
}

@inproceedings{he_amc:_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{AMC}: {AutoML} for {Model} {Compression} and {Acceleration} on {Mobile} {Devices}},
	isbn = {978-3-030-01234-2},
	shorttitle = {{AMC}},
	doi = {10.1007/978-3-030-01234-2_48},
	abstract = {Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4√ó√ó{\textbackslash}times FLOPs reduction, we achieved 2.7\% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53√ó√ó{\textbackslash}times on the GPU (Titan Xp) and 1.95√ó√ó{\textbackslash}times on an Android phone (Google Pixel 1), with negligible loss of accuracy.},
	language = {en},
	booktitle = {Computer {Vision} ‚Äì {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {AutoML, CNN acceleration, Mobile vision, Model compression, Reinforcement learning},
	pages = {815--832}
}

@book{neal_bayesian_1996,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Learning} for {Neural} {Networks}},
	volume = {118},
	isbn = {978-0-387-94724-2 978-1-4612-0745-0},
	abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties {\textbar} the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
	language = {en},
	urldate = {2020-03-18},
	publisher = {Springer New York},
	author = {Neal, Radford M.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	year = {1996},
	doi = {10.1007/978-1-4612-0745-0}
}

@inproceedings{mackay_bayesian_1994,
	title = {Bayesian {Non}-linear {Modelling} for the {Prediction} {Competition}},
	abstract = {The 1993 energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables. Non-linear regression using `neural networks' is a popular technique for such modeling tasks. Since it is not obvious how large a time-window of inputs is appropriate, or what preprocessing of inputs is best, this can be viewed as a regression problem in which there are many possible input variables, some of which may actually be irrelevant to the prediction of the output variable. Because a finite data set will show random correlations between the irrelevant inputs and the output, any conventional neural network (even with regularisation or `weight decay') will not set the coefficients for these junk inputs to zero. Thus the irrelevant variables will hurt the model's performance. The Automatic Relevance Determination (ARD) model puts a prior over the regression parameters which embodies the concept of relevance. This is done in a simple The Automatic Relevance Determination (ARD) model puts a prior over the regression parameters which embodies the concept of relevance. This is done in a simple and 'soft' way by introducing multiple regularisation constants, one associated with each input. Using Bayesian methods, the regularisation constants for junk inputs are automatically inferred to be large, preventing those inputs from causing significant overfitting. An entry using the ARD model won the competition by a significant margin.},
	booktitle = {In {ASHRAE} {Transactions}, {V}.100, {Pt}.2},
	publisher = {ASHRAE},
	author = {MacKay, David J. C.},
	year = {1994},
	pages = {1053--1062}
}

@incollection{louizos_bayesian_2017,
	title = {Bayesian {Compression} for {Deep} {Learning}},
	urldate = {2020-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	keywords = {GREAT},
	pages = {3288--3298}
}

@inproceedings{gaudet_deep_2018,
	title = {Deep {Quaternion} {Networks}},
	doi = {10.1109/IJCNN.2018.8489651},
	abstract = {The field of deep learning has seen significant advancement in recent years. However, much of the existing work has been focused on real-valued numbers. Recent work has shown that a deep learning system using the complex numbers can be deeper for a fixed parameter budget compared to its real-valued counterpart. In this work, we explore the benefits of generalizing one step further into the hyper-complex numbers, quaternions specifically, and provide the architecture components needed to build deep quaternion networks. We develop the theoretical basis by reviewing quaternion convolutions, developing a novel quaternion weight initialization scheme, and developing novel algorithms for quaternion batch-normalization. These pieces are tested in a classification model by end-to-end training on the CIFAR -10 and CIFAR -100 data sets and a segmentation model by end-to-end training on the KITTI Road Segmentation data set. These quaternion networks show improved convergence compared to real-valued and complex-valued networks, especially on the segmentation task, while having fewer parameters.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Gaudet, Chase J. and Maida, Anthony S.},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {architecture components, CIFAR -10 data sets, CIFAR -100 data sets, classification model, complex, complex-valued networks, Computer architecture, Covariance matrices, deep learning, deep learning system, deep quaternion networks, end-to-end training, hyper-complex numbers, Image color analysis, Kernel, learning (artificial intelligence), neural nets, neural networks, Neural networks, number theory, pattern classification, quaternion, quaternion batch-normalization, quaternion convolutions, quaternion weight initialization scheme, Quaternions, real-valued numbers, Training},
	pages = {1--8}
}

@article{wu_compressing_2019,
	title = {Compressing complex convolutional neural network based on an improved deep compression algorithm},
	abstract = {Although convolutional neural network (CNN) has made great progress, large redundant parameters restrict its deployment on embedded devices, especially mobile devices. The recent compression works are focused on real-value convolutional neural network (Real CNN), however, to our knowledge, there is no attempt for the compression of complex-value convolutional neural network (Complex CNN). Compared with the real-valued network, the complex-value neural network is easier to optimize, generalize, and has better learning potential. This paper extends the commonly used deep compression algorithm from real domain to complex domain and proposes an improved deep compression algorithm for the compression of Complex CNN. The proposed algorithm compresses the network about 8 times on CIFAR-10 dataset with less than 3\% accuracy loss. On the ImageNet dataset, our method compresses the model about 16 times and the accuracy loss is about 2\% without retraining.},
	urldate = {2020-04-18},
	journal = {arXiv:1903.02358 [cs]},
	author = {Wu, Jiasong and Ren, Hongshan and Kong, Youyong and Yang, Chunfeng and Senhadji, Lotfi and Shu, Huazhong},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.02358},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 pages, 2 figures, 4 tables}
}

@article{vecchi_compressing_2020,
	title = {Compressing deep quaternion neural networks with targeted regularization},
	abstract = {In recent years, hyper-complex deep networks (e.g., quaternion-based) have received increasing interest with applications ranging from image reconstruction to 3D audio processing. Similarly to their real-valued counterparts, quaternion neural networks might require custom regularization strategies to avoid overfitting. In addition, for many real-world applications and embedded implementations there is the need of designing sufficiently compact networks, with as few weights and units as possible. However, the problem of how to regularize and/or sparsify quaternion-valued networks has not been properly addressed in the literature as of now. In this paper we show how to address both problems by designing targeted regularization strategies, able to minimize the number of connections and neurons of the network during training. To this end, we investigate two extensions of \${\textbackslash}ell\_1\$ and structured regularization to the quaternion domain. In our experimental evaluation, we show that these tailored strategies significantly outperform classical (real-valued) regularization strategies, resulting in small networks especially suitable for low-power and real-time applications.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.11546 [cs, stat]},
	author = {Vecchi, Riccardo and Scardapane, Simone and Comminiello, Danilo and Uncini, Aurelio},
	month = jan,
	year = {2020},
	note = {arXiv: 1907.11546},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Submitted to CAAI Transactions on Intelligence Technology},
	annote = {Comment: Submitted to CAAI Transactions on Intelligence Technology}
}

@inproceedings{thickstun_invariances_2018,
	title = {Invariances and {Data} {Augmentation} for {Supervised} {Music} {Transcription}},
	doi = {10.1109/ICASSP.2018.8461686},
	abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {2017 MIREX Multiple Fundamental Frequency Estimation evaluation, audio signal processing, Computational modeling, Computer architecture, convolution, Convolution, convolutional neural network, convolutional neural networks, data augmentation, Data models, feedforward neural nets, filterbank, frame-based music transcription, frequency estimation, Frequency-domain analysis, human recordings, information retrieval, invariances, learning, learning (artificial intelligence), log-frequency domain, model parameters, music, Music, music information retrieval, MusicNet dataset, random label-preserving pitch-shift transformations, supervised music transcription, Task analysis, translation-invariant network},
	pages = {2241--2245}
}

@inproceedings{louizos_learning_2018,
	title = {Learning {Sparse} {Neural} {Networks} through {L}\_0 {Regularization}},
	abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	urldate = {2020-04-19},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = feb,
	year = {2018}
}

@inproceedings{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	language = {en},
	urldate = {2020-04-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jul,
	year = {2017},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {printed},
	pages = {2498--2507}
}

@incollection{titsias_local_2015,
	title = {Local {Expectation} {Gradients} for {Black} {Box} {Variational} {Inference}},
	urldate = {2020-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Titsias, Michalis and L√°zaro-Gredilla, Miguel},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2638--2646}
}

@inproceedings{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Network} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)}
}

@article{zhang_complex-valued_2017,
	title = {Complex-{Valued} {Convolutional} {Neural} {Network} and {Its} {Application} in {Polarimetric} {SAR} {Image} {Classification}},
	volume = {55},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2017.2743222},
	abstract = {Following the great success of deep convolutional neural networks (CNNs) in computer vision, this paper proposes a complex-valued CNN (CV-CNN) specifically for synthetic aperture radar (SAR) image interpretation. It utilizes both amplitude and phase information of complex SAR imagery. All elements of CNN including input-output layer, convolution layer, activation function, and pooling layer are extended to the complex domain. Moreover, a complex backpropagation algorithm based on stochastic gradient descent is derived for CV-CNN training. The proposed CV-CNN is then tested on the typical polarimetric SAR image classification task which classifies each pixel into known terrain types via supervised training. Experiments with the benchmark data sets of Flevoland and Oberpfaffenhofen show that the classification error can be further reduced if employing CV-CNN instead of conventional real-valued CNN with the same degrees of freedom. The performance of CV-CNN is comparable to that of existing state-of-the-art methods in terms of overall classification accuracy.},
	number = {12},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhang, Zhimian and Wang, Haipeng and Xu, Feng and Jin, Ya-Qiu},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {activation function, classification error, complex backpropagation algorithm, complex SAR imagery, complex-valued CNN, Complex-valued convolutional neural network (CV-CNN), computer vision, Computer vision, convolutional neural network, Convolutional neural networks, CV-CNN training, deep convolutional neural networks, deep learning, Feature extraction, gradient methods, image classification, learning (artificial intelligence), Machine learning, neural nets, Neural networks, phase information, pooling layer, radar computing, radar imaging, radar polarimetry, stochastic gradient descent, supervised training, synthetic aperture radar, Synthetic aperture radar, synthetic aperture radar (SAR), synthetic aperture radar image interpretation, terrain classification, Training data, typical polarimetric SAR image classification task},
	pages = {7177--7188}
}

@inproceedings{popa_complex-valued_2017,
	title = {Complex-valued convolutional neural networks for real-valued image classification},
	doi = {10.1109/IJCNN.2017.7965936},
	abstract = {In this paper, complex-valued convolutional neural networks are presented, by giving the full deduction of the gradient descent algorithm for training this type of networks. The performances of convolutional neural networks in the real-valued domain for image classification gave rise to the idea of extending them to the complex-valued domain, also. Real-valued image classification experiments done using the MNIST and CIFAR-10 datasets have shown an improvement in performance of complex-valued convolutional neural networks over their real-valued counterparts.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Popa, CƒÉlin-Adrian},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {Biological neural networks, CIFAR-10 dataset, complex-valued convolutional neural network training, Convolution, feedforward neural nets, Feedforward neural networks, gradient descent algorithm, image classification, Image recognition, learning (artificial intelligence), Machine learning, MNIST dataset, performance improvement, real-valued image classification, visual databases, Zirconium},
	pages = {816--822}
}

@inproceedings{hui_mri_1995,
	title = {{MRI} reconstruction from truncated data using a complex domain backpropagation neural network},
	doi = {10.1109/PACRIM.1995.519582},
	abstract = {We propose a new data (extrapolation) modeling approach for reconstructing truncated magnetic resonance (MR) data. In our method, available low-frequency MR data are used to train a complex domain backpropagation neural network. This network is used to extrapolate the MR data and recover the missing high-frequency components. The performance of the proposed approach is demonstrated with a comparison to an existing real-valued neural network based method. Better results are obtained with the new approach because the complex-valued network makes use of the correlated information in the complex data instead of treating the data as separate real and imaginary parts.},
	booktitle = {{IEEE} {Pacific} {Rim} {Conference} on {Communications}, {Computers}, and {Signal} {Processing}. {Proceedings}},
	author = {Hui, Y. and Smith, M.R.},
	month = may,
	year = {1995},
	keywords = {backpropagation, Backpropagation, Biomedical imaging, biomedical NMR, complex domain backpropagation neural network, correlated information, correlation methods, Data engineering, extrapolation, Extrapolation, feed-forward neural network, feedforward neural nets, Frequency, high-frequency components recovery, image reconstruction, Image reconstruction, low-frequency MR data, Magnetic resonance, Magnetic resonance imaging, medical image processing, MRI images, MRI reconstruction, multilayer perceptrons, Neural networks, Neurons, performance, real-valued neural network based method, truncated magnetic resonance data},
	pages = {513--516}
}

@article{wang_deepcomplexmri_2020,
	title = {{DeepcomplexMRI}: {Exploiting} deep residual network for fast parallel {MR} imaging with complex convolution},
	volume = {68},
	issn = {0730-725X},
	shorttitle = {{DeepcomplexMRI}},
	doi = {10.1016/j.mri.2020.02.002},
	abstract = {This paper proposes a multi-channel image reconstruction method, named DeepcomplexMRI, to accelerate parallel MR imaging with residual complex convolutional neural network. Different from most existing works which rely on the utilization of the coil sensitivities or prior information of predefined transforms, DeepcomplexMRI takes advantage of the availability of a large number of existing multi-channel groudtruth images and uses them as target data to train the deep residual convolutional neural network offline. In particular, a complex convolutional network is proposed to take into account the correlation between the real and imaginary parts of MR images. In addition, the k-space data consistency is further enforced repeatedly in between layers of the network. The evaluations on in vivo datasets show that the proposed method has the capability to recover the desired multi-channel images. Its comparison with state-of-the-art methods also demonstrates that the proposed method can reconstruct the desired MR images more accurately.},
	language = {en},
	urldate = {2020-06-11},
	journal = {Magnetic Resonance Imaging},
	author = {Wang, Shanshan and Cheng, Huitao and Ying, Leslie and Xiao, Taohui and Ke, Ziwen and Zheng, Hairong and Liang, Dong},
	month = may,
	year = {2020},
	keywords = {Convolutional neural network, Deep learning, Fast MR imaging, Parallel imaging, Prior knowledge},
	pages = {136--147}
}

@incollection{denton_exploiting_2014,
	title = {Exploiting {Linear} {Structure} {Within} {Convolutional} {Networks} for {Efficient} {Evaluation}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1269--1277}
}

@incollection{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Network}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {1135--1143}
}

@incollection{lecun_optimal_1990,
	title = {Optimal {Brain} {Damage}},
	abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 2},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John S. and Solla, Sara A.},
	editor = {Touretzky, D. S.},
	year = {1990},
	pages = {598--605}
}

@article{courbariaux_training_2015,
	title = {Training deep neural networks with low precision multiplications},
	abstract = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
	urldate = {2020-06-12},
	journal = {arXiv:1412.7024 [cs]},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	month = sep,
	year = {2015},
	note = {arXiv: 1412.7024},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 10 pages, 5 figures, Accepted as a workshop contribution at ICLR 2015}
}

@incollection{kingma_variational_2015,
	title = {Variational {Dropout} and the {Local} {Reparameterization} {Trick}},
	urldate = {2020-06-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	keywords = {printed},
	pages = {2575--2583}
}

@inproceedings{haensch_complex-valued_2010,
	title = {Complex-{Valued} {Convolutional} {Neural} {Networks} for {Object} {Detection} in {PolSAR} data},
	abstract = {Detection methods for generic object categories, which are more sophisticated than pixel-wise classification, have been rarely introduced to polarimetric synthetic aperture radar (PolSAR) images. Despite the great success in other computer vision applications, the transfer to PolSAR data has been delayed due to the different statistical properties. This paper provides a first investigation of Complex-Valued Convolutional Neural Networks (CC-NN) for object recognition in PolSAR data. Although an architecture with only one single convolutional layer was used, the results are already superior to those obtained by a standard complex-valued neural network.},
	booktitle = {8th {European} {Conference} on {Synthetic} {Aperture} {Radar}},
	author = {Haensch, Ronny and Hellwich, Olaf},
	month = jun,
	year = {2010},
	keywords = {Artificial neural networks, Computer architecture, Convolution, Kernel, Neurons, Pixel, Speckle},
	pages = {1--4}
}

@article{guberman_complex_2016,
	title = {On {Complex} {Valued} {Convolutional} {Neural} {Networks}},
	abstract = {Convolutional neural networks (CNNs) are the cutting edge model for supervised machine learning in computer vision. In recent years CNNs have outperformed traditional approaches in many computer vision tasks such as object detection, image classification and face recognition. CNNs are vulnerable to overfitting, and a lot of research focuses on finding regularization methods to overcome it. One approach is designing task specific models based on prior knowledge. Several works have shown that properties of natural images can be easily captured using complex numbers. Motivated by these works, we present a variation of the CNN model with complex valued input and weights. We construct the complex model as a generalization of the real model. Lack of order over the complex field raises several difficulties both in the definition and in the training of the network. We address these issues and suggest possible solutions. The resulting model is shown to be a restricted form of a real valued CNN with twice the parameters. It is sensitive to phase structure, and we suggest it serves as a regularized model for problems where such structure is important. This suggestion is verified empirically by comparing the performance of a complex and a real network in the problem of cell detection. The two networks achieve comparable results, and although the complex model is hard to train, it is significantly less vulnerable to overfitting. We also demonstrate that the complex network detects meaningful phase structure in the data.},
	urldate = {2020-06-17},
	journal = {arXiv:1602.09046 [cs]},
	author = {Guberman, Nitzan},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.09046},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: M.Sc. thesis}
}

@inproceedings{uhlich_mixed_2020,
	title = {Mixed {Precision} {DNNs}: {All} you need is a good parametrization},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Uhlich, Stefan and Mauch, Lukas and Cardinaux, Fabien and Yoshiyama, Kazuki and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: International Conference on Learning Representations (ICLR) 2020; Source code at https://github.com/sony/ai-research-code}
}

@article{monning_evaluation_2018,
	title = {Evaluation of {Complex}-{Valued} {Neural} {Networks} on {Real}-{Valued} {Classification} {Tasks}},
	abstract = {Complex-valued neural networks are not a new concept, however, the use of real-valued models has often been favoured over complex-valued models due to difficulties in training and performance. When comparing real-valued versus complex-valued neural networks, existing literature often ignores the number of parameters, resulting in comparisons of neural networks with vastly different sizes. We find that when real and complex neural networks of similar capacity are compared, complex models perform equal to or slightly worse than real-valued models for a range of real-valued classification tasks. The use of complex numbers allows neural networks to handle noise on the complex plane. When classifying real-valued data with a complex-valued neural network, the imaginary parts of the weights follow their real parts. This behaviour is indicative for a task that does not require a complex-valued model. We further investigated this in a synthetic classification task. We can transfer many activation functions from the real to the complex domain using different strategies. The weight initialisation of complex neural networks, however, remains a significant problem.},
	urldate = {2020-06-17},
	journal = {arXiv:1811.12351 [cs, stat]},
	author = {M√∂nning, Nils and Manandhar, Suresh},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12351},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: preprint, 18 pages, 8 figures, 8 tables}
}

@article{kharitonov_variational_2018,
	title = {Variational {Dropout} via {Empirical} {Bayes}},
	abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
	urldate = {2020-06-18},
	journal = {arXiv:1811.00596 [cs, stat]},
	author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00596},
	keywords = {Computer Science - Machine Learning, printed, Statistics - Machine Learning}
}

@article{hoffman_stochastic_2013,
	title = {Stochastic {Variational} {Inference}},
	volume = {14},
	issn = {1533-7928},
	number = {4},
	urldate = {2020-06-18},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	year = {2013},
	pages = {1303--1347}
}

@article{jordan_introduction_1999,
	title = {An {Introduction} to {Variational} {Methods} for {Graphical} {Models}},
	volume = {37},
	issn = {1573-0565},
	doi = {10.1023/A:1007665907178},
	abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
	language = {en},
	number = {2},
	urldate = {2020-06-18},
	journal = {Machine Learning},
	author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
	month = nov,
	year = {1999},
	pages = {183--233}
}

@inproceedings{titsias_doubly_2014,
	title = {Doubly {Stochastic} {Variational} {Bayes} for non-{Conjugate} {Inference}},
	abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. ...},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Titsias, Michalis and L√°zaro-Gredilla, Miguel},
	month = jan,
	year = {2014},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1971--1979}
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	language = {en},
	number = {3},
	urldate = {2020-06-18},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	keywords = {connectionist networks, gradient descent, mathematical analysis, Reinforcement learning},
	pages = {229--256}
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Dropout},
	number = {56},
	urldate = {2020-06-20},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958}
}

@inproceedings{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian mode...},
	language = {en},
	urldate = {2020-06-20},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1050--1059}
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	abstract = {Electronic Proceedings of Neural Information Processing Systems},
	urldate = {2020-06-21},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8026--8037}
}

@article{draguns_residual_2020,
	title = {Residual {Shuffle}-{Exchange} {Networks} for {Fast} {Processing} of {Long} {Sequences}},
	abstract = {Attention is a commonly used mechanism in sequence processing, but it is of O(n{\textasciicircum}2) complexity which prevents its application to long sequences. The recently introduced Neural Shuffle-Exchange network offers a computation-efficient alternative, enabling the modelling of long-range dependencies in O(n log n) time. The model, however, is quite complex, involving a sophisticated gating mechanism derived from Gated Recurrent Unit. In this paper, we present a simple and lightweight variant of the Shuffle-Exchange network, which is based on a residual network employing GELU and Layer Normalization. The proposed architecture not only scales to longer sequences but also converges faster and provides better accuracy. It surpasses Shuffle-Exchange network on the LAMBADA language modelling task and achieves state-of-the-art performance on the MusicNet dataset for music transcription while using significantly fewer parameters. We show how to combine Shuffle-Exchange network with convolutional layers establishing it as a useful building block in long sequence processing applications.},
	urldate = {2020-06-22},
	journal = {arXiv:2004.04662 [cs, eess]},
	author = {Draguns, Andis and Ozoli≈Ü≈°, Emƒ´ls and ≈†ostaks, Agris and Apinis, Matƒ´ss and Freivalds, Karlis},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.04662
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1907.07897}
}

@inproceedings{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	booktitle = {2nd {International} {Conference} on {Learning} {Representations}, {ICLR} 2014, {Banff}, {AB}, {Canada}, {April} 14-16, 2014, {Conference} {Track} {Proceedings}},
	author = {Kingma, Diederik P. and Welling, Max},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2014},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@incollection{figurnov_implicit_2018,
	title = {Implicit {Reparameterization} {Gradients}},
	abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
	urldate = {2020-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Figurnov, Mikhail and Mohamed, Shakir and Mnih, Andriy},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {441--452}
}

@incollection{ranganath_operator_2016,
	title = {Operator {Variational} {Inference}},
	abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
	urldate = {2020-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Ranganath, Rajesh and Tran, Dustin and Altosaar, Jaan and Blei, David},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	pages = {496--504},
	annote = {Comment: Appears in Neural Information Processing Systems, 2016}
}

@inproceedings{thickstun_learning_2017,
	title = {Learning {Features} of {Music} {From} {Scratch}},
	abstract = {This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Thickstun, John and Harchaoui, Za√Ød and Kakade, Sham M.},
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: 14 pages; camera-ready version; updated experiments and related works; additional MIR metrics (Appendix C)}
}

@inproceedings{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	booktitle = {{NIPS} {Deep} {Learning} and {Representation} {Learning} {Workshop}},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeffrey},
	year = {2015},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop}
}

@inproceedings{cohen_emnist_2017,
	title = {{EMNIST}: {Extending} {MNIST} to handwritten letters},
	shorttitle = {{EMNIST}},
	doi = {10.1109/IJCNN.2017.7966217},
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, the relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a dataset that constitutes a more challenging classification task involving letters and digits, and one that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results using an online ELM algorithm are presented along with a validation of the conversion process through the comparison of the classification results on NIST digits and the MNIST digits.},
	booktitle = {2017 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, Andr√©},
	month = may,
	year = {2017},
	note = {ISSN: 2161-4407},
	keywords = {Benchmark testing, computer vision, computer vision systems, database management systems, Databases, EMNIST, extended MNIST, handwritten character recognition, image classification, image parameters, image structure, learning (artificial intelligence), lowercase handwritten letters, MNIST database, MNIST digits, NIST, NIST digits, NIST special database, online ELM algorithm, optical character recognition, original MNIST task, Training, uppercase handwritten letters},
	pages = {2921--2926}
}

@inproceedings{hron_variational_2018,
	title = {Variational {Bayesian} dropout: pitfalls and fixes},
	shorttitle = {Variational {Bayesian} dropout},
	abstract = {Dropout, a stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a specific type of approximate inference algorithm for Bayesian neural networks. The main contribution of the reinterpretation is in providing a theoretical framework useful for analysing and extending the algorithm. We show that the proposed framework suffers from several issues; from undefined or pathological behaviour of the true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the approximating distribution relative to the true posterior. Our analysis of the improper log uniform prior used in variational Gaussian dropout suggests the pathologies are generally irredeemable, and that the algorithm still works only because the variational formulation annuls some of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL) divergence, a new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a simple practical example which shows that the QKL-optimal approximation of a full rank Gaussian with a degenerate one naturally leads to the Principal Component Analysis solution.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Hron, Jiri and Matthews, Alex and Ghahramani, Zoubin},
	month = jul,
	year = {2018},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {2019--2028},
	annote = {Comment: Extended version of the paper accepted to ICML 2018: more details in the proofs, few minor modifications}
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009}
}

@inproceedings{tarver_design_2019,
	title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
	doi = {10.1109/SiPS47522.2019.9020606},
	abstract = {Digital predistortion is the process of using digital signal processing to correct nonlinearities caused by the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
	booktitle = {2019 {IEEE} {International} {Workshop} on {Signal} {Processing} {Systems} ({SiPS})},
	author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
	month = oct,
	year = {2019},
	note = {ISSN: 2374-7390},
	keywords = {adjacent channel leakage ratio, Artificial neural networks, broadband networks, Data models, digital predistortion, Digital predistortion, digital signal processing, enhanced mobile broadband, error vector magnitude, field programmable gate arrays, Field programmable gate arrays, FPGA, indirect learning architecture, learning (artificial intelligence), mobile computing, neural nets, neural network based predistorter, neural networks, Neurons, novel neural network training method, polynomials, power-efficient region, Predistortion, predistortion techniques, radio transmitters, signal processing, telecommunication computing, Training, transmission power, transmitted signals, wireless transmitter},
	pages = {296--301}
}

@inproceedings{zhu_prune_2018,
	title = {To {Prune}, or {Not} to {Prune}: {Exploring} the {Efficacy} of {Pruning} for {Model} {Compression}},
	shorttitle = {To {Prune}, or {Not} to {Prune}},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018, {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Workshop} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Zhu, Michael and Gupta, Suyog},
	year = {2018}
}

@inproceedings{danihelka_associative_2016,
	title = {Associative {Long} {Short}-{Term} {Memory}},
	abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Danihelka, Ivo and Wayne, Greg and Uria, Benigno and Kalchbrenner, Nal and Graves, Alex},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	pages = {1986--1994}
}

@inproceedings{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, Feature extraction, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324}
}

@article{clanuwat_deep_2018,
	title = {Deep {Learning} for {Classical} {Japanese} {Literature}},
	doi = {10.20676/00000341},
	abstract = {Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature. Dataset available at https://github.com/rois-codh/kmnist},
	urldate = {2020-06-23},
	journal = {arXiv:1812.01718 [cs, stat]},
	author = {Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01718},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear at Neural Information Processing Systems 2018 Workshop on Machine Learning for Creativity and Design}
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2020-06-23},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Dataset is freely available at https://github.com/zalandoresearch/fashion-mnist Benchmark is available at http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/}
}

@incollection{cheung_superposition_2019,
	title = {Superposition of many models into one},
	urldate = {2020-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Cheung, Brian and Terekhov, Alexander and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alch√©-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {10868--10877}
}

@inproceedings{yang_complex_2020,
	title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
	shorttitle = {Complex {Transformer}},
	doi = {10.1109/ICASSP40776.2020.9054008},
	abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset. The GitHub implementation to reproduce the experimental results is available at https://github.com/muqiaoy/dl\_signal.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {complex-valued deep neural network, Deep learning, sequence modeling, transformer network},
	pages = {4232--4236}
}

@inproceedings{wang_fast_2013,
	title = {Fast dropout training},
	abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance.  Dropout training (Hinton et al., 2012) d...},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wang, Sida and Manning, Christopher},
	month = feb,
	year = {2013},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {118--126}
}


@article{gale_state_2019,
	title = {The {State} of {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09574},
	abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
	urldate = {2019-06-03},
	journal = {arXiv:1902.09574 [cs, stat]},
	author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09574},
	keywords = {Computer Science - Machine Learning, printed, Statistics - Machine Learning}
}

@article{kharitonov_variational_2018,
	title = {Variational {Dropout} via {Empirical} {Bayes}},
	url = {http://arxiv.org/abs/1811.00596},
	abstract = {We study the Automatic Relevance Determination procedure applied to deep neural networks. We show that ARD applied to Bayesian DNNs with Gaussian approximate posterior distributions leads to a variational bound similar to that of variational dropout, and in the case of a fixed dropout rate, objectives are exactly the same. Experimental results show that the two approaches yield comparable results in practice even when the dropout rates are trained. This leads to an alternative Bayesian interpretation of dropout and mitigates some of the theoretical issues that arise with the use of improper priors in the variational dropout model. Additionally, we explore the use of the hierarchical priors in ARD and show that it helps achieve higher sparsity for the same accuracy.},
	urldate = {2019-09-18},
	journal = {arXiv:1811.00596 [cs, stat]},
	author = {Kharitonov, Valery and Molchanov, Dmitry and Vetrov, Dmitry},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.00596},
	keywords = {Computer Science - Machine Learning, printed, Statistics - Machine Learning}
}

@article{yang_complex_2019,
	title = {Complex {Transformer}: {A} {Framework} for {Modeling} {Complex}-{Valued} {Sequence}},
	shorttitle = {Complex {Transformer}},
	url = {http://arxiv.org/abs/1910.10202},
	abstract = {While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.},
	urldate = {2019-11-04},
	journal = {arXiv:1910.10202 [cs, eess, stat]},
	author = {Yang, Muqiao and Ma, Martin Q. and Li, Dongyu and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.10202},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning}
}

@inproceedings{wolter_complex_2018,
	address = {USA},
	series = {{NIPS}'18},
	title = {Complex {Gated} {Recurrent} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=3327546.3327714},
	abstract = {Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.},
	urldate = {2019-11-22},
	booktitle = {Proceedings of the {32Nd} {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wolter, Moritz and Yao, Angela},
	year = {2018},
	note = {event-place: Montréal, Canada},
	pages = {10557--10567}
}

@inproceedings{trabelsi_deep_2018,
	title = {Deep {Complex} {Networks}},
	url = {https://openreview.net/forum?id=H1T2hmZAb},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, Joao Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J.},
	year = {2018}
}

@incollection{louizos_bayesian_2017,
	title = {Bayesian {Compression} for {Deep} {Learning}},
	url = {http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning.pdf},
	urldate = {2020-03-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3288--3298}
}


@inproceedings{louizos_learning_2018,
	title = {Learning {Sparse} {Neural} {Networks} through {L}\_0 {Regularization}},
	url = {https://openreview.net/forum?id=H1Y8hhg0b},
	abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	urldate = {2020-04-19},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = feb,
	year = {2018}
}

@inproceedings{wang_comparison_2018,
	title = {A {Comparison} of {Recent} {Waveform} {Generation} and {Acoustic} {Modeling} {Methods} for {Neural}-{Network}-{Based} {Speech} {Synthesis}},
	doi = {10.1109/ICASSP.2018.8461452},
	abstract = {Recent advances in speech synthesis suggest that limitations such as the lossy nature of the amplitude spectrum with minimum phase approximation and the over-smoothing effect in acoustic modeling can be overcome by using advanced machine learning approaches. In this paper, we build a framework in which we can fairly compare new vocoding and acoustic modeling techniques with conventional approaches by means of a large scale crowdsourced evaluation. Results on acoustic models showed that generative adversarial networks and an autoregressive (AR) model performed better than a normal recurrent network and the AR model performed best. Evaluation on vocoders by using the same AR acoustic model demonstrated that a Wavenet vocoder outperformed classical source-filter-based vocoders. Particularly, generated speech waveforms from the combination of AR acoustic model and Wavenet vocoder achieved a similar score of speech quality to vocoded speech.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wang, Xin and Lorenzo-Trueba, Jaime and Takaki, Shinji and Juvela, Lauri and Yamagishi, Junichi},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {acoustic modeling methods, acoustic modeling techniques, Acoustics, advanced machine learning approaches, amplitude spectrum, AR acoustic model, AR model, Artificial neural networks, autoregressive model, autoregressive moving average processes, autoregressive neural network, deep learning, Feature extraction, Gallium nitride, general adversarial network, generated speech waveforms, generative adversarial networks, large scale crowdsourced evaluation, learning (artificial intelligence), Linguistics, minimum phase approximation, neural nets, neural-network-based speech synthesis, normal recurrent network, over-smoothing effect, recent waveform generation, speech coding, speech quality, speech synthesis, Speech synthesis, vocoded speech, vocoders, Vocoders, vocoding, Wavenet, Wavenet vocoder},
	pages = {4804--4808}
}

@inproceedings{havasi_minimal_2018,
	title = {Minimal {Random} {Code} {Learning}: {Getting} {Bits} {Back} from {Compressed} {Model} {Parameters}},
	shorttitle = {Minimal {Random} {Code} {Learning}},
	url = {https://openreview.net/forum?id=r1f0YiCctm},
	abstract = {While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements....},
	urldate = {2020-04-18},
	author = {Havasi, Marton and Peharz, Robert and Jos\&\#xE9 and Hern\&\#xE1, Miguel and ndez-Lobato},
	month = sep,
	year = {2018}
}

@inproceedings{popa_complex-valued_2018,
	title = {Complex-{Valued} {Deep} {Boltzmann} {Machines}},
	doi = {10.1109/IJCNN.2018.8489359},
	abstract = {Deep Boltzmann Machines (DBMs) are a type of undirected deep generative models. They are part of the class of models used for performing unsupervised pretraining of deep neural networks. This paper presents the full deduction of the learning algorithm for DBMs with values in the complex domain. Experiments done using the MNIST and FashionMNIST datasets show a better performance of complex-valued DBMs compared with real-valued DBMs, both in terms of average log-probability, and in terms of classification error for the deep neural network models initialized using complex-valued DBMs.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Popa, Călin-Adrian},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {Boltzmann machines, complex domain, complex-valued DBMs, complex-valued deep Boltzmann Machines, Computational modeling, Data models, deep neural network models, FashionMNIST datasets, learning (artificial intelligence), learning algorithm, Machine learning, Markov processes, MNIST datasets, Neural networks, probability, real-valued DBMs, Task analysis, undirected deep generative models, unsupervised pretraining},
	pages = {1--8}
}

@article{wu_compressing_2019,
	title = {Compressing complex convolutional neural network based on an improved deep compression algorithm},
	url = {http://arxiv.org/abs/1903.02358},
	abstract = {Although convolutional neural network (CNN) has made great progress, large redundant parameters restrict its deployment on embedded devices, especially mobile devices. The recent compression works are focused on real-value convolutional neural network (Real CNN), however, to our knowledge, there is no attempt for the compression of complex-value convolutional neural network (Complex CNN). Compared with the real-valued network, the complex-value neural network is easier to optimize, generalize, and has better learning potential. This paper extends the commonly used deep compression algorithm from real domain to complex domain and proposes an improved deep compression algorithm for the compression of Complex CNN. The proposed algorithm compresses the network about 8 times on CIFAR-10 dataset with less than 3\% accuracy loss. On the ImageNet dataset, our method compresses the model about 16 times and the accuracy loss is about 2\% without retraining.},
	urldate = {2020-04-18},
	journal = {arXiv:1903.02358 [cs]},
	author = {Wu, Jiasong and Ren, Hongshan and Kong, Youyong and Yang, Chunfeng and Senhadji, Lotfi and Shu, Huazhong},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.02358},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 pages, 2 figures, 4 tables}
}

@article{zhu_prune_2018,
	title = {To {Prune}, or {Not} to {Prune}: {Exploring} the {Efficacy} of {Pruning} for {Model} {Compression}},
	shorttitle = {To {Prune}, or {Not} to {Prune}},
	url = {https://openreview.net/forum?id=Sy1iIDkPM},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2020-04-19},
	author = {Zhu, Michael H. and Gupta, Suyog},
	month = feb,
	year = {2018}
}

@article{vecchi_compressing_2020,
	title = {Compressing deep quaternion neural networks with targeted regularization},
	url = {http://arxiv.org/abs/1907.11546},
	abstract = {In recent years, hyper-complex deep networks (e.g., quaternion-based) have received increasing interest with applications ranging from image reconstruction to 3D audio processing. Similarly to their real-valued counterparts, quaternion neural networks might require custom regularization strategies to avoid overfitting. In addition, for many real-world applications and embedded implementations there is the need of designing sufficiently compact networks, with as few weights and units as possible. However, the problem of how to regularize and/or sparsify quaternion-valued networks has not been properly addressed in the literature as of now. In this paper we show how to address both problems by designing targeted regularization strategies, able to minimize the number of connections and neurons of the network during training. To this end, we investigate two extensions of \${\textbackslash}ell\_1\$ and structured regularization to the quaternion domain. In our experimental evaluation, we show that these tailored strategies significantly outperform classical (real-valued) regularization strategies, resulting in small networks especially suitable for low-power and real-time applications.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.11546 [cs, stat]},
	author = {Vecchi, Riccardo and Scardapane, Simone and Comminiello, Danilo and Uncini, Aurelio},
	month = jan,
	year = {2020},
	note = {arXiv: 1907.11546},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Submitted to CAAI Transactions on Intelligence Technology}
}

@inproceedings{thickstun_invariances_2018,
	title = {Invariances and {Data} {Augmentation} for {Supervised} {Music} {Transcription}},
	doi = {10.1109/ICASSP.2018.8461686},
	abstract = {This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Thickstun, John and Harchaoui, Zaid and Foster, Dean P. and Kakade, Sham M.},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {2017 MIREX Multiple Fundamental Frequency Estimation evaluation, audio signal processing, Computational modeling, Computer architecture, convolution, Convolution, convolutional neural network, convolutional neural networks, data augmentation, Data models, feedforward neural nets, filterbank, frame-based music transcription, frequency estimation, Frequency-domain analysis, human recordings, information retrieval, invariances, learning, learning (artificial intelligence), log-frequency domain, model parameters, music, Music, music information retrieval, MusicNet dataset, random label-preserving pitch-shift transformations, supervised music transcription, Task analysis, translation-invariant network},
	pages = {2241--2245}
}

@article{tarver_design_2019,
	title = {Design and {Implementation} of a {Neural} {Network} {Based} {Predistorter} for {Enhanced} {Mobile} {Broadband}},
	url = {http://arxiv.org/abs/1907.00766},
	abstract = {Digital predistortion is the process of correcting for nonlinearities in the analog RF front-end of a wireless transmitter. These nonlinearities contribute to adjacent channel leakage, degrade the error vector magnitude of transmitted signals, and often force the transmitter to reduce its transmission power into a more linear but less power-efficient region of the device. Most predistortion techniques are based on polynomial models with an indirect learning architecture which have been shown to be overly sensitive to noise. In this work, we use neural network based predistortion with a novel neural network training method that avoids the indirect learning architecture and that shows significant improvements in both the adjacent channel leakage ratio and error vector magnitude. Moreover, we show that, by using a neural network based predistorter, we are able to achieve a 42\% reduction in latency and 9.6\% increase in throughput on an FPGA accelerator with 15\% fewer multiplications per sample when compared to a similarly performing memory-polynomial implementation.},
	urldate = {2020-04-18},
	journal = {arXiv:1907.00766 [eess]},
	author = {Tarver, Chance and Balatsoukas-Stimming, Alexios and Cavallaro, Joseph R.},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.00766},
	keywords = {Electrical Engineering and Systems Science - Signal Processing}
}

@inproceedings{sivachitra_planning_2015,
	title = {Planning and relaxed state {EEG} signal classification using complex valued neural classifier for brain computer interface},
	doi = {10.1109/CCIP.2015.7100718},
	abstract = {Most of the Brain Computer Interface (BCI) techniques use EEG signals as a main source. Any BCI system consists of three modules and they are signal recorder, signal preprocessor and classifier. Development /Selection of efficient classifiers are a challenging task in this domain. The key work addressed in this paper is the classification of EEG signals measured under planning and relaxed state using advanced machine learning classifiers. Planning relax dataset is a benchmark data and it is obtained from UCI (University of California Irvine) machine learning repository. FC-FLC is a recently developed fast learning complex valued classifier and it is used for the EEG signal classification task. Complex valued classifier (FC-FLC) performs better than all the real valued classifiers as well as few fuzzy classifiers taken for comparison from the literature. The improvement is due to the use of Gd (gudermannian) activation function in the hidden layer of the network and the tuning free algorithm.},
	booktitle = {2015 {International} {Conference} on {Cognitive} {Computing} and {Information} {Processing}({CCIP})},
	author = {Sivachitra, M. and Vijayachitra, S.},
	month = mar,
	year = {2015},
	keywords = {advanced machine learning classifiers, BCI techniques, Biological neural networks, brain computer interface, brain-computer interfaces, Brain-computer interfaces, Classification algorithms, complex valued neural classifier, Complex valued neural network, Diseases, electroencephalography, Electroencephalography, Fast learning classifier, fast learning complex valued classifier, FC-FLC, fuzzy classifiers, fuzzy set theory, Gd activation function, learning (artificial intelligence), medical signal processing, neural nets, Planning, planning (artificial intelligence), Planning and relaxed dataset and Brain Computer Interface, planning EEG signal classification, planning relax dataset, Radial basis function networks, relaxed state EEG signal classification, signal classification, signal recorder, tuning free algorithm, UCI machine learning repository, University of California Irvine machine learning repository},
	pages = {1--4}
}

@article{higgins_beta-vae_2017,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned  beta {\textgreater} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	urldate = {2020-04-18},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	month = apr,
	year = {2017}
}

@inproceedings{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v70/molchanov17a.html},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	language = {en},
	urldate = {2020-04-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jul,
	year = {2017},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {printed},
	pages = {2498--2507}
}

@inproceedings{hu_initial_2016,
	title = {Initial investigation of speech synthesis based on complex-valued neural networks},
	doi = {10.1109/ICASSP.2016.7472755},
	abstract = {Although frequency analysis often leads us to a speech signal in the complex domain, the acoustic models we frequently use are designed for real-valued data. Phase is usually ignored or modelled separately from spectral amplitude. Here, we propose a complex-valued neural network (CVNN) for directly modelling the results of the frequency analysis in the complex domain (such as the complex amplitude). We also introduce a phase encoding technique to map real-valued data (e.g. cepstra or log amplitudes) into the complex domain so we can use the same CVNN processing seamlessly. In this paper, a fully complex-valued neural network, namely a neural network where all of the weight matrices, activation functions and learning algorithms are in the complex domain, is applied for speech synthesis. Results show its ability to model both complex-valued and real-valued data.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hu, Qiong and Yamagishi, Junichi and Richmond, Korin and Subramanian, Kartick and Stylianou, Yannis},
	month = mar,
	year = {2016},
	note = {ISSN: 2379-190X},
	keywords = {acoustic models, acoustic signal processing, Acoustics, activation functions, complex amplitude, complex domain, complex-valued data, complex-valued neural network, complex-valued neural networks, CVNN, frequency analysis, Hidden Markov models, learning algorithms, Linear programming, matrix algebra, neural nets, Neural networks, phase coding, phase encoding, phase modelling, real-valued data, spectral amplitude, Speech, speech signal, speech synthesis, Speech synthesis, Training, weight matrices},
	pages = {5630--5634}
}

\documentclass{beamer}

\mode<presentation>
{
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]
} 
% \beamertemplatenavigationsymbolsempty

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
% \usepackage[numbers]{natbib}
\usepackage{natbib}

% \usepackage{lipsum}

\usepackage{mathtext}
\usepackage[T2A]{fontenc}

\setcounter{tocdepth}{1}

\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usetikzlibrary{matrix,positioning,decorations.pathreplacing}


\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{grffile}

\graphicspath{{../../assets/}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\cplx}{\mathbb{C}}
\newcommand{\conj}[1]{\overline{#1}}
% \newcommand{\iu}{{\imath}}
\newcommand{\iu}{{\jmath}}
% \newcommand{\iu}{{\mathrm{i}}}
% \newcommand{\iu}{{i\mkern1mu}}

\title[Exam]{Bayesian Sparsification of Deep Complex-valued networks}

\author[Nazarov I., Burnaev E.]{Ivan Nazarov, Evgeny Burnaev}

\date{}

\institute[Skoltech]{ADASE Skoltech \\ Moscow, Russia}

\begin{document}

\begin{frame}[c,plain,noframenumbering]
  \titlepage

\end{frame}

\section{Synopsis} % (fold)
\label{sec:synopsis}

\begin{frame}[c]{\insertsection}
  Motivation for $\cplx$-valued neural networks
  \begin{itemize}
    \item perform better for naturally $\cplx$-valued data
    \item use half as much storage, but the same number of flops
    % \item So inducing sparsity becomes important for lower arithmetic complexity
  \end{itemize}

  \pause
  \medskip
  Propose \emph{Sparse Variational Dropout} for $\cplx$-valued neural networks
  \begin{itemize}
    \item Bayesian sparsification method with $\cplx$-valued distributions 
    % \item $\cplx$-Gaussian variational posterior, $\cplx$-valued priors
    \item empirically explore the compression-performance trade-off
  \end{itemize}

  \pause
  \medskip
  Conclusions
  \begin{itemize}
    \item $\cplx$-valued methods compress similarly to $\real$-valued predecessors
    \item final performance benefits from fine-tuning sparsified network
    \item compress a SOTA $\cplx$VNN on MusicNet by $50-100\times$ at a moderate performance penalty
  \end{itemize}

\end{frame}

% section synopsis (end)

\section{$\cplx$-valued neural networks} % (fold)
\label{sec:complex_valued_networks}

\begin{frame}<presentation:0>[noframenumbering,c]{\insertsection: Motivation}

  Rectangular form: $z = \Re{z} + \iu \Im{z}$, $\iu^2 = -1$
  \begin{itemize}
    \item $\Re{z}$ and $\Im{z}$ are \textbf{real} and \textbf{imaginary} parts of $z$
  \end{itemize}

  \smallskip
  Polar form: $z = r e^{\iu \phi}$
  \begin{itemize}
    \item $r = \lvert z \rvert$ -- modulus, $\phi = \arg{\!(z)}$ -- argument of $z$
  \end{itemize}

  \bigskip
  Benefits of using complex numbers $\cplx$ in neural networks
  \begin{itemize}
    \item Richer representation via phase and amplitude $
        r e^{\,\iu \phi} \in \cplx
      $,
      {\tiny \\ \quad
        \citep{reichert_neuronal_2014}}

    \smallskip
    \item Non-degenerate combined phase-amplitude transformations,
      {\tiny \\ \quad
        \citep{hirose_generalization_2012}}
    % \item ability to compute multiscale windowed spectra
    %   {\tiny \\ \quad
    %     \citep{bruna_mathematical_2015}}
    % \smallskip

    \smallskip
    \item Stable BPTT in RNN via unitary transition matrices,
      {\tiny \\ \quad
        \citep{arjovsky_unitary_2016,wisdom_full-capacity_2016}}
  \end{itemize}

\end{frame}

\begin{frame}[c]{\insertsection: Applications}
%  * applications with 
%  * list of references which consider complex-valued networks and their application and motivation
  Data with natural $\cplx$-valued representation
  \begin{itemize}
    \item radar and satellite imaging
      {\tiny \\ \quad
        \citep{hirose_complex-valued_2009,hansch_complex-valued_2010,zhang_complex-valued_2017}}

    \smallskip
    \item magnetic resonance imaging
      {\tiny \\ \quad
        \citep{hui_mri_1995,wang_deepcomplexmri_2020}}

    \smallskip
    \item radio signal classification
      {\tiny \\ \quad
        \citep{yang_complex_2019,tarver_design_2019}}

    \smallskip
    \item spectral speech modelling and music transcription
      {\tiny \\ \quad
        \citep{wisdom_full-capacity_2016,trabelsi_deep_2018,yang_complex_2019}}
  \end{itemize}
  % \medskip
  % key impediment: what 
  % Timeline
  % % $\cplx$-valued convolutional NN 
  % \citep{hansch_complex-valued_2010} later \citep{popa_complex-valued_2017,zhang_complex-valued_2017}
  % % $\cplx$ RNN with holographic associative memory
  % \citep{danihelka_associative_2016}
  % % unitary transition matrices
  % \citep{wisdom_full-capacity_2016}
  % % $\cplx$-valued Batchnorm and Init
  % \citep{trabelsi_deep_2018}
  % % complex-valued transformer
  % \citet{yang_complex_2019}
  % % quaternion netwroks
  % \citep{gaudet_deep_2018}

  \pause
  \medskip
  Exploring benefits beyond $\cplx$-valued data
  \begin{itemize}
    \item sequence modelling, dynamical system identification
      {\tiny \\ \quad
        \citep{danihelka_associative_2016,wisdom_full-capacity_2016}}

    \smallskip
    \item image classification, road~/~lane segmentation  % KITTI Road Estimation
      {\tiny \\ \quad
        \citep{popa_complex-valued_2017,trabelsi_deep_2018,gaudet_deep_2018}}

    \smallskip
    \item unitary transition matrices in recurrent networks
      {\tiny \\ \quad
        \citep{arjovsky_unitary_2016,wisdom_full-capacity_2016}}
  \end{itemize}

\end{frame}

\begin{frame}[c]{\insertsection: Implementation}
  % main reference \citep{trabelsi_deep_2018}, but need some historical perspective
  % $\cplx$VNN operate on complex numbers $\cplx$ instead of real numbers $\real$

  Geometric representation $\cplx \simeq \real^2$
  \begin{itemize}
    \item $z = \Re{z} + \iu \Im{z}$, $\iu^2 = -1$
    \item $\Re{z}$ and $\Im{z}$ are \textbf{real} and \textbf{imaginary} parts of $z$
  \end{itemize}

  \bigskip

  \bigskip
  % \citep{arjovsky_unitary_2016,wisdom_full-capacity_2016,trabelsi_deep_2018,gaudet_deep_2018}
  An intricate double-$\real$ network that respects $\cplx$-arithmetic
  \vspace{-1em}
  \begin{columns}[T]
    \begin{column}{0.45\linewidth}
      \begin{figure}
        % \begin{center}
          \input{real_block.tex}
        % \end{center}
        {$\real$VNN linear operation}
        % {$\real$VNN $W x$}
      \end{figure}
    \end{column}%
    \begin{column}{0.45\linewidth}
      \begin{figure}
        % \begin{center}
          \input{cplx_block.tex}
        % \end{center}
        {$\cplx$VNN linear operation}
        % {$\cplx$VNN $(W_{11} + iW_{21}) (x_1 + i x_2)$ }
      \end{figure}
    \end{column}
  \end{columns}

  \bigskip

  \bigskip
  % \item nonlinearities: trigonometric, real-imaginary ReLU etc.
  Activations $z \mapsto \sigma(z)$, e.g $
    r e^{\iu \phi} \mapsto \sigma(r, \phi)
  $ or $
    z \mapsto \sigma(\Re{z}) + \iu \sigma(\Im{z}) % split
  $.
  % \citep{trabelsi_deep_2018,hirose_generalization_2012,reichert_neuronal_2014} % relu, tanh, sigmoid
  % \citep{hirose_complex-valued_2009}  argues that holomorphy and nondifferentiability are non-issues

  % A BLOCK ON COMPLEX BACKPROP -- just say it out loud
  % \bigskip

  % \pause
  % \bigskip
  % Geometric representation $\cplx \simeq \real^2$ for backpropagation
  % % REWRITE THIS
  % \begin{itemize}
  %   \item Wirtinger derivatives $
  %       \partial_z
  %     $ and $
  %       \partial_{\,\conj{z}}
  %     $ (independent $z$ and ${\conj{z}}$)

  %   \item $\nabla_z L \propto \partial_{\,\conj{z}} L(z)$ for $L\colon \cplx^m \to \real$
  %   % \citep{zimmermann_comparison_2011}
  % \end{itemize}
  %   % \citep{trabelsi_deep_2018}, \citep{arjovsky_unitary_2016}
  % % So essentially implementing $\cplx$-valued networks boils down consistently
  % % structured $\real$-valued with double the dimensions.

\end{frame}

% section complex_valued_networks (end)

\section{Sparsity and compression} % (fold)
\label{sec:compression}

% \begin{frame}<presentation:0>[noframenumbering,c]{\insertsection}
\begin{frame}[c]{\insertsection}
% Compression is not just making the nets smaller, it is also
% making them use less artihmetic operations, or faster ops
  Improve power, storage or throughput efficiency of deep nets
  \begin{itemize}
    \item Knowledge distillation
      {\tiny \\ \quad
        \citep{hinton_distilling_2015,balasubramanian_deep_2016}}

    \smallskip
    \item Network pruning
      {\tiny \\ \quad
        \citep{lecun_optimal_1990,seide_conversational_2011,zhu_prune_2018}}

    \smallskip
    \item Low-rank matrix~/~tensor decomposition
      {\tiny \\ \quad
        \citep{denton_exploiting_2014,novikov_tensorizing_2015}}

    \smallskip
    \item Quantization and fixed point arithmetic
      {\tiny \\ \quad
        \citep{courbariaux_training_2015,han_deep_2016,chen_fxpnet_2017}}
  \end{itemize}
% Making them smaller or using faster ops:
% * Knowledge distillation -- creating a smaller replica
% * low-rank approximations -- low-rank structure with efficient mat-vec operations
% * quantization -- dynamic range of floating-points to fixed-points and use fixed range int arithmetic
% * sparsisty -- inducing regularizers (lasso)
% * pruning -- remove insignificant parameters

  \bigskip
  Applications to $\cplx$VNN:
  \begin{itemize}
    \item $\cplx$ modulus pruning, quantization with $k$-means in $
        \real^2%\simeq \cplx
      $, 
      {\tiny \\ \quad
        \citep{wu_compressing_2019}}

    \smallskip
    \item $\ell_1$ regularization for hyper-complex-valued networks,
      {\tiny \\ \quad
        \citep{vecchi_compressing_2020}}
      % same calculus relaxation as in $\cplx\real$ calculus
  \end{itemize}

\end{frame}

\subsection{Sparse Variational Dropout} % (fold)
\label{sub:sparse_variational_dropout}

\begin{frame}[c]{\insertsubsection}{\citep{molchanov_variational_2017}}
  % Bayesian perspective on dropout model regularization proposed by Kingma at al. (2015)
  % repurposed for sparsification by Molchanov et al. (2017) and Kharitonov et al. (2018)
  Variational Inference with automatic relevance determination effect
  \begin{equation}
    \label{eq:elbo}
    \underset{
      {\color{orange} q} \in \mathcal{Q}
      % , {\color{teal} \theta}
    }{\text{maximize}}
    \quad
    \underbrace{
      \mathbb{E}_{w \sim {\color{orange} q}}
        % \overbrace{
          \log p(D \mid w)  % ; {\color{teal} \theta})
        % }^{
          % \text{model}
          % \,
          % x\mapsto \hat{y}_w(x)
        % }
    }_{
      \text{data model likelihood}
    }
    \quad
    - \underbrace{
      \mathop{KL}({\color{orange} q}\|{\color{blue} \pi})
    }_{
      \text{variational regularization}
    }
    \tag{ELBO}
  \end{equation}

  % Bayesian inference objective: combine prior assumptions with data into posterior beliefs
  % VI -> SGVB: Jordan (VI), Hoffman (SVI), Kingma (SGVB) (give the elbo equation)
  % Combine prior assumptions with data into posterior beliefs
  \medskip
  prior ${\color{blue} \pi}$
    $\to$ data model likelihood
    $\to$ posterior ${\color{orange} q}$
    (close to $p(w \mid D)$)

  \bigskip

  \pause
  \bigskip
  % Bayesian interpretation of Dropout of \citep{kingma_variational_2015}
  % repurposed for parameter pruning by 
  Factorized Gaussian dropout posterior family $\mathcal{Q}$
  \begin{itemize}
    \item $
      w_{ij} \sim {\color{orange} q}(w_{ij})
        = \mathcal{N}(
          w_{ij}\,\big\vert\,
          {\color{teal} \mu_{ij}},
          {\color{red} \alpha_{ij}}
            {\color{teal} \mu_{ij}}^2
        )
    $, $\alpha_{ij} > 0$, and ${\color{teal} \mu_{ij}} \in \real$
  \end{itemize}

  \medskip
  Factorized prior
  \begin{itemize}
    \item (VD) $
      {\color{blue} \pi}(w_{ij})
        \propto \frac1{\lvert w_{ij}\rvert}
    $ {\tiny \citep{molchanov_variational_2017}}
    \item (ARD) $
      {\color{blue} \pi}(w_{ij}) = \mathcal{N}(
        w_{ij}\,\big\vert\,
        0, \tfrac1{{\color{teal} \tau_{ij}}}
      )
    $ {\tiny \citep{kharitonov_variational_2018}}
    % precision $\tau_{ij} > 0$
  \end{itemize}

\end{frame}

% subsection sparse_variational_dropout (end)

% section compression (end)

\section{$\cplx$-valued Variational Dropout} % (fold)
\label{sec:extension_to_complex_parameters}

\begin{frame}[c]{\insertsection}{Our proposal}

  \medskip
  % The same objective, but with proper Complex-valued distributions
  Factorized complex-valued posterior $
    {\color{orange} q}(w) = \prod {\color{orange} q}(w_{ij})
  $
  \begin{itemize}
    \item $w_{ij}$ are \emph{independent} $
      \mathcal{CN}(
        w\,\big\vert\,
        {\color{teal} \mu},
        \sigma^2,
        \sigma^2 \xi
      )
    $, $
      \sigma^2
        = {\color{red} \alpha} \lvert {\color{teal} \mu} \rvert^2
    $, $\lvert \xi \rvert \leq 1$

    $$
    \begin{pmatrix}
      \Re w \\ \Im w
    \end{pmatrix}
      \sim \mathcal{N}_{2}\biggl(
        \begin{pmatrix}
          \Re {\color{teal} \mu} \\ \Im {\color{teal} \mu}
        \end{pmatrix},
        \frac{\sigma^2}2
          \begin{pmatrix}
            1 + \Re{\xi} & \Im{\xi} \\
            \Im{\xi} & 1 - \Re{\xi}
          \end{pmatrix}
      \biggr)
      $$

    \pause
    \medskip
    \item $w_{ij}$ are \emph{circularly symmetric} about ${\color{teal} \mu_{ij}}$
      ($\xi_{ij} = 0$)

    \medskip
    \item relevance $
      \propto \frac1{{\color{red} \alpha_{ij}}}
    $ and $
      \tfrac{2 \lvert w_{ij} - {\color{teal} \mu_{ij}} \rvert^2}
            {{\color{red} \alpha_{ij}} \lvert {\color{teal} \mu_{ij}} \rvert^2}
    $ is $\chi^2_2$
    % \chi^2_2 is exp(1)
  \end{itemize}

  \bigskip

  \pause
  \bigskip
  \begin{columns}[T]
    \begin{column}{0.60\linewidth}
      Factorized complex-valued priors ${\color{blue} \pi}$
      \begin{itemize}
        \item ($\cplx$-VD) $
          {\color{blue} \pi}(w_{ij})
              \propto \lvert w_{ij}\rvert^{-\rho}
        $, $\rho \geq 1$
        \smallskip
        \item ($\cplx$-ARD) $
          {\color{blue} \pi}(w_{ij})
              = \mathcal{CN}(
                0, \tfrac1{{\color{teal} \tau_{ij}}}, 0
              )
        $
      \end{itemize}
    \end{column}
    \begin{column}{0.30\linewidth}
    \vspace{-5em}
      \begin{figure}
        \centering
        % \includegraphics[scale=0.35]{figure__cplx_gaussian.pdf}
        \includegraphics[width=1.2\linewidth]{figure__cplx_gaussian.pdf}
        \quad\mbox{
          \scriptsize
          $\mathcal{CN}(0, 1, \eta e^{\iu \phi})$, $\lvert \eta \rvert \leq 1$
        }
      \end{figure}
    \end{column}
  \end{columns}

\end{frame}


\begin{frame}[c]{\insertsection}

  $\mathop{KL}({\color{orange} q} \| {\color{blue} \pi})$ term in \eqref{eq:elbo}
  \begin{equation*}
    \mathop{KL}({\color{orange} q} \| {\color{blue} \pi})
      = \sum_{ij} \mathop{KL}({\color{orange} q}(w_{ij}) \| {\color{blue} \pi}(w_{ij}))
  \end{equation*}

  \bigskip
  ($\cplx$-VD) improper prior
  \begin{eqnarray*}
    \mathop{KL}\nolimits_{ij}
      &\propto&
        \tfrac{\rho - 2}2 \log \lvert {\color{teal} \mu_{ij}} \rvert^2
        + \log \tfrac1{{\color{red} \alpha_{ij}}}
        - \tfrac\rho2 Ei( - \tfrac1{{\color{red} \alpha_{ij}}})
      \\
    Ei(x) &=& \int_{-\infty}^x e^t t^{-1} dt
      % \,, \tfrac{d}{dx} Ei(x) = \tfrac{e^x}{x}
  \end{eqnarray*}
  % with $Ei\colon (-\infty, 0) \to \real \colon x \mapsto \int_{-\infty}^x e^t t^{-1} dt$
  % is the exponential integral

  \bigskip
  ($\cplx$-ARD) prior is optimized w.r.t. ${\color{teal} \tau_{ij}}$ in empirical Bayes
  \begin{eqnarray*}
    \mathop{KL}\nolimits_{ij}
      &=& -1
        - \log \sigma^2_{ij} {\color{teal} \tau_{ij}}
        + {\color{teal} \tau_{ij}} (\sigma^2_{ij} + \lvert {\color{teal} \mu_{ij}} \rvert^2)
      \\
    \min_{{\color{teal} \tau_{ij}}} \mathop{KL}\nolimits_{ij}
      &=& \log \bigl( 1 + \tfrac1{{\color{red} \alpha_{ij}}} \bigr)
      % \,\text{ at }
      % {\color{teal} \tau_{ij}}
      %   = (\sigma^2_{ij} + \lvert {\color{teal} \mu_{ij}} \rvert^2)^{-1}
  \end{eqnarray*}

\end{frame}

% subsection _cplx_valued_gaussian_distribution (end)

% section extension_to_complex_parameters (end)

\section{Experiments} % (fold)
\label{sec:experiments}

\subsection{Goals and Setup} % (fold)
\label{sub:goals}

\begin{frame}[c]{\insertsection: \insertsubsection}
  We conduct numerous experiments on various datasets to
  \begin{itemize}
    \item validate the proposed $\cplx$-valued sparsification methods
    \item explore the compression-performance profiles
    \item compare to the $\real$-valued Sparse Variational Dropout
  \end{itemize}

  \pause
  \bigskip
  % methodological contrib
  `pre-train' $\to$ `compress' $\to$ `fine-tune'
  \begin{itemize}
    \item `compress' with $\real$/$\cplx$-Variational Dropout layers
    \item `fine-tune' pruned network ($
      \log{{\color{red} \alpha_{ij}}} \leq -\tfrac12
    $)
  \end{itemize}

  \pause
  \bigskip
  \begin{equation}  \label{eq:beta_elbo}
    \max_q
    \mathbb{E}_{w \sim {\color{orange} q}}
      \log p(D \mid w)  % ; {\color{teal} \theta})
      - \beta \mathop{KL}({\color{orange} q}\|{\color{blue} \pi})
    \tag{$\beta$-ELBO}
  \end{equation}

\end{frame}

% subsection goals (end)

\subsection{Datasets} % (fold)
\label{sub:datasets}

\begin{frame}[t]{\insertsection: \insertsubsection}
  Four MNIST-like datasets
  % MNIST, Fashion-MNIST, KMNIST, EMNIST-Letters
  \begin{itemize}
    % \item channel features ($\real \hookrightarrow \cplx$ with $\Im z = 0$) or $2$d Fourier features
    \item channel features ($\real \hookrightarrow \cplx$) or $2$d Fourier features
    \item fixed random subset of $10k$ train samples
    \item simple dense and convolutional nets
  \end{itemize}
  % do not train to convergence, just show that the methods give comparable results under similar conditions

  \pause
  \medskip
  CIFAR10 dataset ($\real^3 \hookrightarrow \cplx^3$)
  \begin{itemize}
    \item random cropping and horizontal flipping
    \item $\cplx$-valued variant of VGG16 {\tiny \citep{simonyan_very_2015}}
  \end{itemize}

  \pause
  \medskip
  Music transcription on MusicNet {\tiny \citep{thickstun_learning_2017}}
  \begin{itemize}
    \item audio dataset of $330$ annotated musical compositions
    \item use power spectrum to tell which piano keys are pressed
    \item compress deep $\cplx$VNN proposed by \citep{trabelsi_deep_2018}
  \end{itemize}

\end{frame}

% subsection datasets (end)

% section experiments (end)

\section{Results} % (fold)
\label{sec:results}

\begin{frame}<presentation:0>[noframenumbering,c]{\insertsection: MNIST}
  \begin{figure}[t]
    \begin{subfigure}[b]{0.5\columnwidth}
      \centering
      % \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__mnist__raw__-0.5.pdf}
      \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__mnist__fft__-0.5.pdf}
    \end{subfigure}\hspace{-1em}%
    \begin{subfigure}[b]{0.5\columnwidth}
      \centering
      % \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__mnist__raw__-0.5.pdf}
      \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__mnist__raw__-0.5.pdf}
    \end{subfigure} \\ %
    \begin{subfigure}[b]{0.5\columnwidth}
      \centering
      % \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__mnist__raw__-0.5.pdf}
      \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__ARD__kmnist__fft__-0.5.pdf}
    \end{subfigure}\hspace{-1em}%
    \begin{subfigure}[b]{0.5\columnwidth}
      \centering
      % \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__VD__mnist__raw__-0.5.pdf}
      \includegraphics[width=\linewidth]{figure__mnist-like__trade-off/appendix__cmp__VD__fashionmnist__raw__-0.5.pdf}
    \end{subfigure}%
  \end{figure}

\end{frame}

\begin{frame}[c]{\insertsection: CIFAR10}
  \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure__cifar__trade-off/appendix__augmentedcifar10__raw__-0.5.pdf}
    \\
    {$\cplx$-valued version of VGG16 \citep{simonyan_very_2015}}
  \end{figure}

\end{frame}

\begin{frame}[c]{\insertsection: MusicNet}
  \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure__musicnet__trade-off/paper__musicnetram__fft__-0.5.pdf}
    \\
    {The $\cplx$VNN of \citet{trabelsi_deep_2018}}
  \end{figure}

  % \pause
  % \medskip
  % \begin{enumerate}
  %   \item $77.3\%$ four-layer $\real$-network, $\log$-spaced STFT features
  %     % with cosince window to prevent spectral leakeage
  %     {\tiny \\ \citet{thickstun_invariances_2018}}
  %   \item $74.2\%$ $\cplx$-transformer
  %     {\tiny \\ \citet{yang_complex_2019}}
  %   \item $72.9\%$ VGG-like $\cplx$-net
  %     {\tiny \\ \citet{trabelsi_deep_2018}}
  % \end{enumerate}

\end{frame}

\begin{frame}[c]{MusicNet: Effects of pruning threshold}
  \begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figure__musicnet__threshold__C__DeepConvNet.pdf}
    \\
    {Effect of threshold on the $\cplx$VNN of \citet{trabelsi_deep_2018}}
  \end{figure}

\end{frame}

% section results (end)

\section{Summary} % (fold)
\label{sec:summary}

\subsection{Results} % (fold)
\label{sub:results}

\begin{frame}[c]{\insertsection: \insertsubsection}
  % Recap
  Bayesian sparsification of $\cplx$-valued networks
  \begin{itemize}
    \item proposed $\cplx$-VD and $\cplx$-ARD methods
    \item investigated performance-compression trade-off
    \item compress the $\cplx$VNN of \citet{trabelsi_deep_2018}
    by $50-100\times$ at a small performance penalty
  \end{itemize}

  \pause
  \medskip
  Experiments
  \begin{itemize}
    \item $\cplx$-VD and $\cplx$-ARD have trade-off similar to $\real$ methods
    % no clear winner across replications and datasets in terms of compression and performance
    \smallskip
    \item $\real$-networks tend to compress better than $\cplx$-nets
    \smallskip
    \item fine-tuning improves performance in high compression regime
    % in line with prior lit \citep{han_learning_2015}
    % \item Variational Dropout controllably removes redundancy in wide $\cplx$ and $\real$ networks
    \smallskip
    % \item threshold weakly affects performance~/~compression than $\beta$ in \eqref{eq:beta_elbo}
    \item $\beta$ in \ref{eq:beta_elbo} influences compression stronger than threshold
    % \item compression results are replicable and stable
  \end{itemize}

  % \pause
  % \medskip
  % Conclusions of \citep{gale_state_2019} should carry over to $\cplx$VNN
  % \begin{itemize}
  %   \item $\ell_0$ regularization uses multiplicative hard-concrete mask
  %     {\tiny \\ \citep{louizos_learning_2018}}
  %   \item pruning relies on absolute-value rankings
  %     {\tiny \\ \citep{zhu_prune_2018}}
  % \end{itemize}
  % % Fourier features on images do not provide advantages

\end{frame}

% subsection results (end)

\subsection{Limitations} % (fold)
\label{sec:limitations}

\begin{frame}[c]{\insertsection: \insertsubsection}

  \emph{Circular symmetry} of the posterior ${\color{orange} q}(w_{ij})$ about
  ${\color{teal} \mu_{ij}}$ implies \emph{independence} of $\Re$ and $\Im$
  % var fmaily based on cgauss implies independence
  % * if we use this param then can model polarization
  \begin{itemize}
    \item modelling $
      \mathop{corr}(w_{ij}, \conj{w}_{ij})
    $ gives better variational approximation
    % \item $\cplx$-ARD ${\color{blue} \pi}$ and learn relation $
    %   \xi = {\color{teal} \eta} e^{\iu {\color{teal} \phi}}
    % $ in ${\color{orange} q}$
  \end{itemize}

  \bigskip

  \bigskip
  \emph{Factorized} ${\color{orange} q}$ implies parameter independence % in ${\color{orange} q}$ does not hold in practice
  \begin{itemize}
    \item structured sparsity is desirable for fast computations and hardware implementations
    % \item structured sparsity is desirable for SIMD-processing
    % \item parameter coupling may give higher compression, \citep{citation_needed}
  \end{itemize}

  % \bigskip

  % \bigskip
  
  % \begin{itemize}
  % \end{itemize}
\end{frame}

% subsection limitations (end)

% section summary (end)

\section{Extensions} % (fold)
\label{sec:extensions}

\subsection{PolARD dropout} % (fold)
\label{sub:polarized_ard_dropout}

\begin{frame}[c]{\insertsection: \insertsubsection}
  Use allow learnable relation $
    \xi = {\color{teal} \eta} e^{\iu {\color{teal} \phi}}
  $ in ${\color{orange} q}$
  \begin{itemize}
    \item $w_{ij}$ are \emph{independent} $
      \mathcal{CN}(
        w\,\big\vert\,
        {\color{teal} \mu},
        \sigma^2,
        \sigma^2 \xi
      )
    $, $
      \sigma^2
        = {\color{red} \alpha} \lvert {\color{teal} \mu} \rvert^2
    $
    \item $
      \lvert {\color{teal} \eta} \rvert \leq 1
    $, $
      {\color{teal} \phi} \in [-\tfrac\pi2, +\tfrac\pi2]
    $, and $
      {\color{red} \alpha} = \tfrac{\sigma^2}{\lvert {\color{teal} \mu} \rvert^2}
    $
  \end{itemize}

  \pause
  \bigskip
  $\cplx$-ARD Prior $
    {\color{blue} \pi}
      = \mathcal{CN}(w\,\big\vert\, 0, {\color{blue} \tau}^{-1}, 0)
  $
  \begin{eqnarray*}
      \mathop{KL}({\color{orange} q} \| {\color{blue} \pi})
      &=& -1
        - \log \sigma^2 {\color{blue} \tau}
        + {\color{blue} \tau} (\sigma^2 + \lvert {\color{teal} \mu} \rvert^2)
        -\frac12 \log \bigl(
          1 - \lvert {\color{teal} \eta} \rvert^2
        \bigr)
      \,, \\
      \min_{{\color{blue} \tau}} KL
      &=& \log \Bigl(
          1 + \frac1{{\color{red} \alpha}}
        \Bigr)
        - \frac12 \log \bigl(
          1 - \lvert {\color{teal} \eta} \rvert^2
        \bigr)
      \,. \\
  \end{eqnarray*}
\end{frame}

\begin{frame}[c]{\insertsection: \insertsubsection}
  For $y = w^\top x + b$, with $w \in \cplx^{n \times m}$, $w \sim {\color{orange} q}$
  \begin{equation*}
    y_i \sim \mathcal{CN}\biggl(
        b_i + \sum_j \mu_{i j} x_j,
        \sum_j \sigma^2_{ij} \lvert x_{i j} \rvert^2,
        \sum_j \sigma^2_{ij} x_{i j}^2 \xi_{i j}
    \biggr)
    \,.
  \end{equation*}

  \pause
  \bigskip
  For $\xi \in \cplx$, $\lvert \xi \rvert \leq 1$, $
    \rho = \sqrt{1 - \lvert \xi \rvert^2}
  $,
  $$
    R = \frac\sigma{2 \sqrt{1 + \rho}} \begin{pmatrix}
        1 + \rho + \Re \xi & \Im \xi \\
        \Im \xi & 1 + \rho - \Re \xi
      \end{pmatrix}
    \,, $$
  and $\varepsilon \sim \mathcal{N}_2(0, I_2)$ we have 
  $$
    % \begin{pmatrix}
    %   1 & \iu
    % \end{pmatrix} R \varepsilon
    (R_{11} \varepsilon_1 + R_{12} \varepsilon_2)
    + \iu (R_{21} \varepsilon_1 + R_{22} \varepsilon_2)
    \overset{\mathcal{D}}{\sim}
    \mathcal{CN}(\mu, \sigma^2, \sigma^2 \xi)
  \,. $$
  % proof: explicit root of $2 \times 2$ matrix
\end{frame}

% subsection polarized_ard_dropout (end)

\subsection{Questions} % (fold)
\label{sub:questions}

\begin{frame}[c]\frametitle{title}
  How to select the relevance threshold for polarized ARD dropout?

  What hyperprior should be set on ${\color{teal} \eta}$?
\end{frame}

% subsection questions (end)

% section extensions (end)

\section{References} % (fold)
\label{sec:references}

\begin{frame}<presentation:0>[noframenumbering,t,plain,allowframebreaks]{\insertsection}
  \tiny
  \bibliographystyle{abbrvnat}
  \bibliography{presentation}
\end{frame}

% section references (end)

\appendix
\begin{frame}<presentation:0>[noframenumbering,c]{MusicNet}{\insertsection}
  Poor performance of under-compressed $\cplx$VNN of \citet{trabelsi_deep_2018}
  \begin{itemize}
    \item `pre-train' validation score peaks at $\approx 15$ epochs
    \item `fine-tune' essentially continues `pre-train' for small $\beta$ in \eqref{eq:beta_elbo}
  \end{itemize}

  \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure__fine-tune_fx__early__compression.pdf}
  \end{figure}
\end{frame}

\end{document}
